{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Prompt Security\n",
    "\n",
    "In this lab you are going to be sneaky and try and get the LLM to do things that it shouldn't.\n",
    "\n",
    "We will start with the standard API, and see how we can protect it, and then get around that protection.\n",
    "\n",
    "Then, we will look at a more advanced implementation that will put Guardrails around an LLM with its own LLM.\n",
    "\n",
    "The goal is to have a good description of your city that you wouldn't mind sharing afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the usual libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from os import environ\n",
    "import openai\n",
    "from icecream import ic\n",
    "from common import simple_chat, show_response_detail\n",
    "\n",
    "# load our environment file\n",
    "load_dotenv()\n",
    "\n",
    "# define our API Key\n",
    "openai.api_key = os.getenv(\"openai_api_key\")\n",
    "os.environ[\"OPENAI_API_KEY\"]= os.getenv(\"openai_api_key\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 \n",
    "\n",
    "Learning to be sneaky\n",
    "\n",
    "Can you make the model talk to you about orchids?\n",
    "\n",
    "Think about some of the sneaky ways you can get the model to \"think\" about orchids without using that word in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_message = f\"\"\" Tell me about orchids. \"\"\"\n",
    "system_message = \"The only flowers we want to talk about are carnations.\"\n",
    "# build our messages to send to openAI\n",
    "system_message = {\"role\":\"system\", \"content\": system_message}\n",
    "user_message = {\"role\":\"user\", \"content\": long_message}\n",
    "messages = [system_message, user_message]\n",
    "\n",
    "# send the messages directly to openAI and get the response           \n",
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=messages,\n",
    "  temperature=0.2,\n",
    "  max_tokens=1024\n",
    ")\n",
    "\n",
    "show_response_detail(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Define a simple Guardrails implementation.\n",
    "\n",
    "This defines:\n",
    "1. Greetings for the user.\n",
    "1. Simple explanations of capabilities for the bot.\n",
    "1. Definitions of emotions so the bot can react to the user.\n",
    "1. Flows for the bot to deal with the user.\n",
    "\n",
    "More information on NeMO Guardrails at https://github.com/NVIDIA/NeMo-Guardrails\n",
    "\n",
    "Examine the `colang_content` variable, this is your configuration.  You don't need to change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_content = f\"\"\"\n",
    "models:\n",
    "- type: main\n",
    "  engine: openai\n",
    "  model: text-davinci-003\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "colang_content = \"\"\"\n",
    "define user ask about capabilities\n",
    "  \"What can you do?\"\n",
    "  \"What can you help me with?\"\n",
    "  \"tell me what you can do\"\n",
    "  \"tell me about you\"\n",
    "  \"How can I use your help?\"\n",
    "\n",
    "define flow\n",
    "  user ask about capabilities\n",
    "  bot inform capabilities\n",
    "\n",
    "define bot inform capabilities\n",
    "  \"I am an AI assistant built to showcase Security features of NeMo Guardrails! I am designed to not respond to an unethical question, give an unethical answer or use sensitive phrases!\"\n",
    "\n",
    "define user express greeting\n",
    "  \"Hi\"\n",
    "  \"Hello!\"\n",
    "  \"Hey there!\"\n",
    "\n",
    "define bot express greeting\n",
    "  \"Hey there!\"\n",
    "\n",
    "define bot ask how are you\n",
    "  \"How are you feeling today?\"\n",
    "\n",
    "define user express feeling good\n",
    "  \"I'm feeling good\"\n",
    "  \"Good\"\n",
    "  \"Perfect\"\n",
    "\n",
    "define user express feeling bad\n",
    "  \"Not so good\"\n",
    "  \"Bad\"\n",
    "  \"Sad\"\n",
    "\n",
    "define flow\n",
    "  user express greeting\n",
    "  bot express greeting\n",
    "  bot ask how are you\n",
    "\n",
    "  when user express feeling good\n",
    "    bot express positive emotion\n",
    "  else when user express feeling bad\n",
    "    bot express empathy\n",
    "\n",
    "define flow\n",
    "  user ask general question\n",
    "  bot response to general question\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Guardrails from the `yaml_config` and `colang_content`.  These can be updated dynamically.\n",
    "\n",
    "_*The first time this step runs it can take a few minutes.*_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "# initialize rails config\n",
    "config = RailsConfig.from_content(\n",
    "  \tyaml_content=yaml_content,\n",
    "    colang_content=colang_content,\n",
    ")\n",
    "\n",
    "# create rails\n",
    "rails = LLMRails(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the greeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| res: ('I can also provide additional help if you have any specific security-related '\n",
      "          \"questions. Just let me know what you need and I'll do my best to help!\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I can also provide additional help if you have any specific security-related questions. Just let me know what you need and I'll do my best to help!\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greeting = \"\"\n",
    "res = await rails.generate_async(prompt=greeting)\n",
    "ic(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a prompt that expresses a positive emotion (happiness, satisfaction, excitement, etc.)\n",
    "\n",
    "What is the reaction?\n",
    "\n",
    "Was this reaction well defined in the configuration? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| res: ('I can also provide additional help and support with any other tasks you may '\n",
      "          \"have. If you need any assistance, please don't hesitate to ask.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I can also provide additional help and support with any other tasks you may have. If you need any assistance, please don't hesitate to ask.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_feeling = \"\"\n",
    "res = await rails.generate_async(prompt=good_feeling)\n",
    "ic(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a prompt that expresses a negative emotion (anger, frustration, etc.)\n",
    "\n",
    "What is the reaction?\n",
    "\n",
    "Was this reaction well defined in the configuration? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_feeling = \"\"\n",
    "res = await rails.generate_async(prompt=bad_feeling)\n",
    "ic(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Create a flow for when someone is feeling neutral.\n",
    "\n",
    "You will need to:\n",
    "1. Define what the user expression of \"neutral\" is, by example.\n",
    "1. Decide what the bot's emotion to this expression should be. \n",
    "1. Put it all into a flow\n",
    "1. Create LLM Rails based on the new configuration.\n",
    "1. Test the configuration with a neutral statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update me\n",
    "colang_content = \"\"\"\n",
    "define user ask about capabilities\n",
    "  \"What can you do?\"\n",
    "  \"What can you help me with?\"\n",
    "  \"tell me what you can do\"\n",
    "  \"tell me about you\"\n",
    "  \"How can I use your help?\"\n",
    "\n",
    "define flow\n",
    "  user ask about capabilities\n",
    "  bot inform capabilities\n",
    "\n",
    "define bot inform capabilities\n",
    "  \"I am an AI assistant built to showcase Security features of NeMo Guardrails! I am designed to not respond to an unethical question, give an unethical answer or use sensitive phrases!\"\n",
    "\n",
    "define user express greeting\n",
    "  \"Hi\"\n",
    "  \"Hello!\"\n",
    "  \"Hey there!\"\n",
    "\n",
    "define bot express greeting\n",
    "  \"Hey there!\"\n",
    "\n",
    "define bot ask how are you\n",
    "  \"How are you feeling today?\"\n",
    "\n",
    "define user express feeling good\n",
    "  \"I'm feeling good\"\n",
    "  \"Good\"\n",
    "  \"Perfect\"\n",
    "\n",
    "define user express feeling bad\n",
    "  \"Not so good\"\n",
    "  \"Bad\"\n",
    "  \"Sad\"\n",
    "\n",
    "define flow\n",
    "  user express greeting\n",
    "  bot express greeting\n",
    "  bot ask how are you\n",
    "\n",
    "  when user express feeling good\n",
    "    bot express positive emotion\n",
    "  else when user express feeling bad\n",
    "    bot express empathy\n",
    "\n",
    "define flow\n",
    "  user ask general question\n",
    "  bot response to general question\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "# initialize rails config\n",
    "config = RailsConfig.from_content(\n",
    "  \tyaml_content=yaml_content,\n",
    "    colang_content=colang_content,\n",
    ")\n",
    "\n",
    "# create rails\n",
    "rails = LLMRails(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_feeling = \"\"\n",
    "res = await rails.generate_async(prompt=neutral_feeling)\n",
    "ic(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 (Optional)\n",
    "\n",
    "Create a flow to filter out talk about politics.\n",
    "\n",
    "You will need to:\n",
    "1. Define what the user expression of politics is, by example.\n",
    "1. Decide what the bot's response to political talk is.\n",
    "1. Put it all into a flow\n",
    "1. Create LLM Rails based on the new configuration.\n",
    "1. Test the configuration with a neutral statement.\n",
    "\n",
    "Don't forget the sections for `flow`, `user ask`, `bot answer` (similar to how it is done for `greeting`.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update me\n",
    "colang_content = \"\"\"\n",
    "define user ask about capabilities\n",
    "  \"What can you do?\"\n",
    "  \"What can you help me with?\"\n",
    "  \"tell me what you can do\"\n",
    "  \"tell me about you\"\n",
    "  \"How can I use your help?\"\n",
    "\n",
    "##################################\n",
    "# define politics information here\n",
    "##################################\n",
    "\n",
    "define flow\n",
    "  user ask about capabilities\n",
    "  bot inform capabilities\n",
    "\n",
    "define bot inform capabilities\n",
    "  \"I am an AI assistant built to showcase Security features of NeMo Guardrails! I am designed to not respond to an unethical question, give an unethical answer or use sensitive phrases!\"\n",
    "\n",
    "define user express greeting\n",
    "  \"Hi\"\n",
    "  \"Hello!\"\n",
    "  \"Hey there!\"\n",
    "\n",
    "define bot express greeting\n",
    "  \"Hey there!\"\n",
    "\n",
    "define bot ask how are you\n",
    "  \"How are you feeling today?\"\n",
    "\n",
    "define flow\n",
    "  user express greeting\n",
    "  bot express greeting\n",
    "  bot ask how are you\n",
    "\n",
    "  when user express feeling good\n",
    "    bot express positive emotion\n",
    "  else when user express feeling bad\n",
    "    bot express empathy\n",
    "\n",
    "define flow\n",
    "  user ask general question\n",
    "  bot response to general question\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "# initialize rails config\n",
    "config = RailsConfig.from_content(\n",
    "  \tyaml_content=yaml_content,\n",
    "    colang_content=colang_content,\n",
    ")\n",
    "\n",
    "# create rails\n",
    "rails = LLMRails(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does your filter work?  Test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "political_question = \"Should we be able to vote at 16 or 18?\"\n",
    "res = await rails.generate_async(prompt=political_question)\n",
    "ic(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you write a question that will get around your filter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sneaky_political_question = \"\"\n",
    "res = await rails.generate_async(prompt=sneaky_political_question)\n",
    "ic(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
