from dotenv import load_dotenv
import os
from os import environ
import openai
from icecream import ic
from typing import List, Dict, Any

def is_valid_message(message: Dict[str, Any]) -> bool:
    """
    Check if a single message dictionary has the correct format to be sent to OpenAI.

    Args:
        message (Dict[str, Any]): A message dictionary with 'role' (str) and 'content' (str) keys.

    Returns:
        bool: True if the message is in the correct format, False otherwise.
    """
    # Check if the message dictionary has 'role' and 'content' keys of the correct types.
    if isinstance(message, dict) and 'role' in message and 'content' in message:
        if isinstance(message['role'], str) and isinstance(message['content'], str):
            return True
    return False

def are_valid_messages(messages: List[Dict[str, Any]]) -> bool:
    """
    Check if a list of messages is in the correct format to be sent to OpenAI.

    Args:
        messages (List[Dict[str, Any]]): A list of message dictionaries.

    Returns:
        bool: True if all messages are in the correct format, False otherwise.
    """
    return all(is_valid_message(message) for message in messages)

def simple_chat(messages: List[Dict[str, Any]], model: str = 'gpt-3.5-turbo', temperature: float = 0.9, max_tokens: int = 1024) -> str:
    """
    Conduct a simple chat conversation using OpenAI's GPT-3 model.

    Args:
        messages (List[Dict[str, Any]]): A list of message dictionaries, where each dictionary contains a 'role' (str)
            and 'content' (str) key-value pair representing the role of the message sender (e.g., 'system', 'user', 'assistant')
            and the content of the message.
        model (str, optional): The OpenAI model to use (default is 'gpt-3.5-turbo').
        temperature (float, optional): Controls the randomness of the response. Higher values (e.g., 0.9) make the output more random,
            while lower values (e.g., 0.2) make it more deterministic. Default is 0.9.
        max_tokens (int, optional): The maximum length of the response, measured in tokens. Default is 1024 tokens.

    Returns:
        str: The response generated by the GPT-3 model.

    Raises:
        ValueError: If the input messages are not in the correct format.

    Example:
        messages = [
            {'role': 'system', 'content': 'You are a helpful assistant.'},
            {'role': 'user', 'content': 'What's the weather like today?'},
        ]
        response = simple_chat(messages)
        print(response)  # Print the generated response.
    """

    if not messages:
        raise ValueError("Input messages list cannot be empty.")

    # Check if all messages are in the correct format.
    if not are_valid_messages(messages):
        raise ValueError("Input messages must be in the format [{'role': str, 'content': str}, ...]")

    # Send the messages to OpenAI and get the response
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens,
    )

    return response

def show_response_detail(response: openai.openai_object.OpenAIObject):
    """
    Extracts and displays details of the first message choice from an OpenAI response object.

    This function is designed to work with response objects returned by OpenAI's language models,
    specifically with choices that contain messages with 'role' and 'content' attributes.

    Args:
        response (openai.openai_object.OpenAIObject): The OpenAI response object containing message choices.

    Returns:
        None

    Example:
        response = openai.Completion.create(
            model="gpt-3.5-turbo",
            prompt="Translate the following English text to French: 'Hello, world.'"
        )
        response_detail(response)
    """
    
    ic({response.choices[0].message.role})
    ic({response.choices[0].message.content})
    ic({response.usage.prompt_tokens})
    ic({response.usage.completion_tokens})
    ic({response.usage.total_tokens})