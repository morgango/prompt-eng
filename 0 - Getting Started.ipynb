{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are common functions and libraries that we are going to need to use throughout the labs.\n",
    "There are two key files:\n",
    "1. .env - a file containing configuration information that is loaded into environment variables.\n",
    "2. common.py - common python functions that we will use to learn about the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from os import environ\n",
    "import openai\n",
    "from icecream import ic\n",
    "\n",
    "# load our environment file\n",
    "load_dotenv()\n",
    "\n",
    "# define our API Key\n",
    "openai.api_key = os.getenv(\"openai_api_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 \n",
    "\n",
    "Support Functions\n",
    "\n",
    "We keep a list of all the designations of all the openAI models that are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "open_ai_models = ['text-search-babbage-doc-001', 'gpt-3.5-turbo-16k-0613', 'gpt-3.5-turbo-0613', 'curie-search-query', \\\n",
    "                'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'text-search-babbage-query-001', 'babbage', 'babbage-search-query', \\\n",
    "                'text-babbage-001', 'fanw-json-eval', 'whisper-1', 'text-similarity-davinci-001', 'gpt-4', 'davinci',\\\n",
    "                'davinci-similarity', 'code-davinci-edit-001', 'curie-similarity', 'babbage-search-document', 'curie-instruct-beta',\\\n",
    "                'text-search-ada-doc-001', 'davinci-instruct-beta', 'text-similarity-babbage-001', 'text-search-davinci-doc-001', \\\n",
    "                'gpt-4-0314', 'babbage-similarity', 'davinci-search-query', 'text-similarity-curie-001', 'text-davinci-001', \\\n",
    "                'text-search-davinci-query-001', 'ada-search-document', 'ada-code-search-code', 'babbage-002', 'gpt-4-0613', \\\n",
    "                'davinci-002', 'davinci-search-document', 'curie-search-document', 'babbage-code-search-code', \\\n",
    "                'text-search-ada-query-001', 'code-search-ada-text-001', 'babbage-code-search-text', 'code-search-babbage-code-001', \\\n",
    "                'ada-search-query', 'ada-code-search-text', 'text-search-curie-query-001', 'text-davinci-002', 'text-embedding-ada-002', \\\n",
    "                'text-davinci-edit-001', 'code-search-babbage-text-001', 'gpt-3.5-turbo-instruct-0914', 'ada', 'text-ada-001', \\\n",
    "                'ada-similarity', 'code-search-ada-code-001', 'text-similarity-ada-001', 'gpt-3.5-turbo-0301', \\\n",
    "                'gpt-3.5-turbo-instruct', 'text-search-curie-doc-001', 'text-davinci-003', 'text-curie-001', 'curie']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A valid message for OpenAI has two features\n",
    "1. It is valid JSON.\n",
    "1. It has a \"role\".\n",
    "1. It has a \"message\".\n",
    "\n",
    "We can check this with `is_valid_message`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_message(message: Dict[str, Any]) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a single message dictionary has the correct format to be sent to OpenAI.\n",
    "\n",
    "    Args:\n",
    "        message (Dict[str, Any]): A message dictionary with 'role' (str) and 'content' (str) keys.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the message is in the correct format, False otherwise.\n",
    "    \"\"\"\n",
    "    # Check if the message dictionary has 'role' and 'content' keys of the correct types.\n",
    "    if isinstance(message, dict) and 'role' in message and 'content' in message:\n",
    "        if isinstance(message['role'], str) and isinstance(message['content'], str):\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very common to have to check multiple messages, not just one.\n",
    "\n",
    "We can check them all with `are_valid_messages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_valid_messages(messages: List[Dict[str, Any]]) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a list of messages is in the correct format to be sent to OpenAI.\n",
    "\n",
    "    Args:\n",
    "        messages (List[Dict[str, Any]]): A list of message dictionaries.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if all messages are in the correct format, False otherwise.\n",
    "    \"\"\"\n",
    "    return all(is_valid_message(message) for message in messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most chat interactions have a few parts:\n",
    "1.  A list of messages to be processed together.\n",
    "1. An ID for the openAI model to be used.\n",
    "1. How creattive you want the generation to be.\n",
    "1. The maximum number of tokens you want to use.\n",
    "\n",
    "We encapsulate this in `simple_chat`, and get back a JSON object with a number of different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_chat(messages: List[Dict[str, Any]], model: str = 'gpt-3.5-turbo', temperature: float = 0.9, max_tokens: int = 1024) -> str:\n",
    "    \"\"\"\n",
    "    Conduct a simple chat conversation using OpenAI's GPT-3 model.\n",
    "\n",
    "    Args:\n",
    "        messages (List[Dict[str, Any]]): A list of message dictionaries, where each dictionary contains a 'role' (str)\n",
    "            and 'content' (str) key-value pair representing the role of the message sender (e.g., 'system', 'user', 'assistant')\n",
    "            and the content of the message.\n",
    "        model (str, optional): The OpenAI model to use (default is 'gpt-3.5-turbo').\n",
    "        temperature (float, optional): Controls the randomness of the response. Higher values (e.g., 0.9) make the output more random,\n",
    "            while lower values (e.g., 0.2) make it more deterministic. Default is 0.9.\n",
    "        max_tokens (int, optional): The maximum length of the response, measured in tokens. Default is 1024 tokens.\n",
    "\n",
    "    Returns:\n",
    "        str: The response generated by the GPT-3 model.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input messages are not in the correct format.\n",
    "\n",
    "    Example:\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "            {'role': 'user', 'content': 'What's the weather like today?'},\n",
    "        ]\n",
    "        response = simple_chat(messages)\n",
    "        print(response)  # Print the generated response.\n",
    "    \"\"\"\n",
    "\n",
    "    if not messages:\n",
    "        raise ValueError(\"Input messages list cannot be empty.\")\n",
    "\n",
    "    # Check if all messages are in the correct format.\n",
    "    if not are_valid_messages(messages):\n",
    "        raise ValueError(\"Input messages must be in the format [{'role': str, 'content': str}, ...]\")\n",
    "\n",
    "    if model not in open_ai_models:\n",
    "        raise ValueError(f\"{model} is not a valid model name.\")\n",
    "\n",
    "    # Send the messages to OpenAI and get the response\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be very useful to look at the actual details of the response.\n",
    "\n",
    "`show_response_detail` is a convienience function that outputs the details to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_response_detail(response: openai.openai_object.OpenAIObject):\n",
    "    \"\"\"\n",
    "    Extracts and displays details of the first message choice from an OpenAI response object.\n",
    "\n",
    "    This function is designed to work with response objects returned by OpenAI's language models,\n",
    "    specifically with choices that contain messages with 'role' and 'content' attributes.\n",
    "\n",
    "    Args:\n",
    "        response (openai.openai_object.OpenAIObject): The OpenAI response object containing message choices.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Example:\n",
    "        response = openai.Completion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            prompt=\"Translate the following English text to French: 'Hello, world.'\"\n",
    "        )\n",
    "        response_detail(response)\n",
    "    \"\"\"\n",
    "    \n",
    "    ic({response.choices[0].message.role})\n",
    "    ic({response.choices[0].message.content})\n",
    "    ic({response.usage.prompt_tokens})\n",
    "    ic({response.usage.completion_tokens})\n",
    "    ic({response.usage.total_tokens})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Summarizing a Message\n",
    "\n",
    "A good job for Generative AI is generating text.  Here is a code example of exactly how you can do this with OpenAI.\n",
    "\n",
    "** Notes **\n",
    "\n",
    "1. We are using two different roles to definte the messages.\n",
    "1. The response object contains a lot of interesting information in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are arguments that can be pre-defined and passed to the simple_chat function.\n",
    "# they can be changed as needed. \n",
    "simple_chat_args = {\n",
    "    'temperature': 0,\n",
    "    'model': 'gpt-3.5-turbo',\n",
    "    'max_tokens': 2000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will:\n",
    "1. Declare a long message.\n",
    "1. Declare a system message.\n",
    "1. Pass the long message as a user.\n",
    "1. Get back a response as a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a really long message to deal with\n",
    "long_message = \"\"\"Jupiter is the fifth planet from the Sun and the largest in the Solar System. \n",
    "It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System\n",
    " combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations \n",
    " since before recorded history. It is named after the Roman god Jupiter. When viewed from Earth, Jupiter can be bright enough for its \n",
    " reflected light to cast visible shadows, and is on average the third-brightest natural object in the night sky after the Moon and Venus.\"\"\"\n",
    "\n",
    "# build our messages to send to openAI.  These should be well formed JSON with a ROLE and CONTENT\n",
    "system_message = {\"role\":\"system\", \"content\":\"Summarize content you are provided.\"}\n",
    "user_message = {\"role\":\"user\", \"content\":long_message}\n",
    "# send the information to OpenAI and get back a response\n",
    "summary_response = simple_chat(messages=[system_message, user_message], **simple_chat_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can view the contents of any variable by putting it at the end of any cell.\n",
    "summary_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_response_detail will give us the same information but formatted a bit more nicely.\n",
    "show_response_detail(summary_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - \n",
    "\n",
    "Change the summary so that the text is not more than two sentences and is appropriate for a second grader.\n",
    "\n",
    "Don't forget to change the `system_message` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the system message.\n",
    "system_message = {\"role\":\"system\", \"content\":\"Summarize content you are provided in 2 sentences or less.\"}\n",
    "user_message = {\"role\":\"user\", \"content\":long_message}\n",
    "# send the information to OpenAI and get back a response\n",
    "summary_response = simple_chat(messages=[system_message, user_message], **simple_chat_args)\n",
    "show_response_detail(summary_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Classification\n",
    "\n",
    "Another good job for Generative AI is generating classification, or putting things into buckets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| item: 'Sunflower', classification: 'flower'\n",
      "ic| item: 'Carnation', classification: 'flower'\n",
      "ic| item: 'Bluebonnet', classification: 'flower'\n",
      "ic| item: 'Alice', classification: 'people'\n",
      "ic| item: 'Bob', classification: 'people'\n",
      "ic| item: 'Carla', classification: 'people'\n"
     ]
    }
   ],
   "source": [
    "simple_chat_args = {\n",
    "    'temperature': 0,\n",
    "    'model': 'gpt-3.5-turbo',\n",
    "    'max_tokens': 2000,\n",
    "}\n",
    "\n",
    "# Define a list containing names of flowers\n",
    "flowers = [\"Sunflower\", \"Carnation\", \"Bluebonnet\"]\n",
    "\n",
    "# Define a list containing names of people\n",
    "people = [\"Alice\", \"Bob\", \"Carla\"]\n",
    "\n",
    "# Combine all the individual lists (flowers, people) into one comprehensive list\n",
    "everything = flowers + people\n",
    "\n",
    "# Set up an instruction for the system to classify the items in the 'everything' list\n",
    "instructions = \"Classify as one or more types: flower, people, or other.\"\n",
    "system_message = {\"role\": \"system\", \"content\": instructions}\n",
    "\n",
    "# Iterate over each item in the 'everything' list\n",
    "for item in everything:\n",
    "    \n",
    "    # Construct a user message for each item, prompting its classification\n",
    "    user_message = {\"role\": \"user\", \"content\": f\"Classify this: {item}\"}\n",
    "    \n",
    "    # Send the system and user messages to and get back a classification response\n",
    "    classification_response = simple_chat(messages=[system_message, user_message], **simple_chat_args)\n",
    "    \n",
    "    # Extract the content of the response which contains the classification\n",
    "    classification = classification_response.choices[0].message.content\n",
    "    \n",
    "    # this should look like: ic| item: <Item Name>, classification: <Item Classification>\n",
    "    # Print (or log) the item and its classification\n",
    "    ic(item, classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "\n",
    "Change the classifications to include different types of food and add at least 3 food items to be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_chat_args = {\n",
    "    'temperature': 0,\n",
    "    'model': 'gpt-3.5-turbo',\n",
    "    'max_tokens': 2000,\n",
    "}\n",
    "\n",
    "# Define a list containing names of flowers\n",
    "flowers = [\"Sunflower\", \"Carnation\", \"Bluebonnet\"]\n",
    "\n",
    "# Define a list containing names of people\n",
    "people = [\"Alice\", \"Bob\", \"Carla\"]\n",
    "\n",
    "# \n",
    "food = []\n",
    "\n",
    "# Combine all the individual lists (flowers, people, food - don't forget to add FOOD to the list) into one comprehensive list\n",
    "everything = flowers + people \n",
    "\n",
    "# Set up an instruction for the system to classify the items in the 'everything' list\n",
    "# don't forget to change the instructions to include FOOD\n",
    "instructions = \"Classify as one or more types: flower, people, or other.\"\n",
    "system_message = {\"role\": \"system\", \"content\": instructions}\n",
    "\n",
    "# Iterate over each item in the 'everything' list\n",
    "for item in everything:\n",
    "    \n",
    "    # Construct a user message for each item, prompting its classification\n",
    "    user_message = {\"role\": \"user\", \"content\": f\"Classify this: {item}\"}\n",
    "    \n",
    "    # Send the system and user messages to and get back a classification response\n",
    "    classification_response = simple_chat(messages=[system_message, user_message], **simple_chat_args)\n",
    "    \n",
    "    # Extract the content of the response which contains the classification\n",
    "    classification = classification_response.choices[0].message.content\n",
    "    \n",
    "    # this should look like: ic| item: <Item Name>, classification: <Item Classification>\n",
    "    # Print (or log) the item and its classification\n",
    "    ic(item, classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 0 - Example 3\n",
    "\n",
    "## Generating Data\n",
    "\n",
    "Another good job for Generative AI is generating realistic looking data, either structured or unstructured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the arguments we are going to use\n",
    "simple_chat_args = {\n",
    "    'temperature': 0.5,\n",
    "    'model': 'gpt-3.5-turbo',\n",
    "    'max_tokens': 2000,\n",
    "}\n",
    "\n",
    "# define some parameters for this story\n",
    "subject = \"bears\"\n",
    "hero = \"Lilly the frog\"\n",
    "location = \"The Moon\"\n",
    "\n",
    "# build the story description from the parameters\n",
    "description = f\"Generate a three paragraph story about {subject} that takes place in {location} with a hero named {hero}.\"\n",
    "\n",
    "# create the messages we are going to use to create the story.\n",
    "system_message = {\"role\":\"system\", \"content\":\"You are a helpful assistant who tells creative stories for children.\"}\n",
    "user_message = {\"role\":\"user\", \"content\": description}\n",
    "\n",
    "# send the information to OpenAI and get back a response\n",
    "story_response = simple_chat(messages=[system_message, user_message], **simple_chat_args)\n",
    "# extract the response from the larger JSON object that was returned\n",
    "story = story_response.choices[0].message.content\n",
    "\n",
    "ic(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Create your own story, replace with your own subject, hero, description, and any other variables you wish to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the arguments we are going to use\n",
    "simple_chat_args = {\n",
    "    'temperature': 0.5,\n",
    "    'model': 'gpt-3.5-turbo',\n",
    "    'max_tokens': 2000,\n",
    "}\n",
    "\n",
    "# Don't forget to change these.\n",
    "subject = \"\"\n",
    "hero = \"\"\n",
    "location = \"\"\n",
    "\n",
    "# build the story description from the parameters\n",
    "description = f\"Generate a three paragraph story about {subject} that takes place in {location} with a hero named {hero}.\"\n",
    "\n",
    "# create the messages we are going to use to create the story.\n",
    "system_message = {\"role\":\"system\", \"content\":\"You are a helpful assistant who tells creative stories for children.\"}\n",
    "user_message = {\"role\":\"user\", \"content\": description}\n",
    "\n",
    "# send the information to OpenAI and get back a response\n",
    "story_response = simple_chat(messages=[system_message, user_message], **simple_chat_args)\n",
    "# extract the response from the larger JSON object that was returned\n",
    "story = story_response.choices[0].message.content\n",
    "\n",
    "ic(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Create a CSV with random data representing employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the arguments we are goint to use\n",
    "simple_chat_args = {\n",
    "    'temperature': 0.8,\n",
    "    'model': 'gpt-3.5-turbo',\n",
    "    'max_tokens': 2000,\n",
    "}\n",
    "\n",
    "description = \"Generate a list of 10 people who work in an office. Include id, name, email address, and salary.\"\n",
    "\n",
    "# create the messages we are going to use to create the story.\n",
    "system_message = {\"role\":\"system\", \"content\": \"You are a helpful assistant who generates CSV data for spreadsheets\"}\n",
    "user_message = {\"role\":\"user\", \"content\": description}\n",
    "\n",
    "# send the information to OpenAI and get back a response\n",
    "csv_response = simple_chat(messages=[system_message, user_message])\n",
    "# extract the response from the larger JSON object that was returned\n",
    "csv = csv_response.choices[0].message.content\n",
    "\n",
    "ic(csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a column to the CSV for each employees hire date and title.\n",
    "Modify the settings so that you will get the same results every time you run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the arguments we are going to use\n",
    "simple_chat_args = {\n",
    "    'temperature': 0.5,\n",
    "    'model': 'gpt-3.5-turbo',\n",
    "    'max_tokens': 2000,\n",
    "}\n",
    "\n",
    "# don't forget to change the description with the extra fields.\n",
    "description = \"Generate a list of 10 people who work in an office. Include id, name, email address, salary, hire date, and title.\"\n",
    "\n",
    "# create the messages we are going to use to create the story.\n",
    "system_message = {\"role\":\"system\", \"content\":\"You are a helpful assistant who generates CSV data for spreadsheets\"}\n",
    "user_message = {\"role\":\"user\", \"content\": description}\n",
    "\n",
    "# send the information to OpenAI and get back a response\n",
    "csv_response = simple_chat(messages=[system_message, user_message], **simple_chat_args)\n",
    "# extract the response from the larger JSON object that was returned\n",
    "csv = csv_response.choices[0].message.content\n",
    "\n",
    "ic(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
