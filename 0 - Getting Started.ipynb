{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are common functions and libraries that we are going to need to use throughout the labs.\n",
    "There are two key files:\n",
    "1. .env - a file containing configuration information that is loaded into environment variables.\n",
    "2. common.py - common python functions that we will use to learn about the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from os import environ\n",
    "import openai\n",
    "from icecream import ic\n",
    "\n",
    "# load our environment file\n",
    "load_dotenv()\n",
    "\n",
    "# define our API Key\n",
    "openai.api_key = os.getenv(\"openai_api_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 0 - Support Functions\n",
    "\n",
    "We keep a list of all the designations of all the openAI models that are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "open_ai_models = ['text-search-babbage-doc-001', 'gpt-3.5-turbo-16k-0613', 'gpt-3.5-turbo-0613', 'curie-search-query', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'text-search-babbage-query-001', 'babbage', 'babbage-search-query', 'text-babbage-001', 'fanw-json-eval', 'whisper-1', 'text-similarity-davinci-001', 'gpt-4', 'davinci', 'davinci-similarity', 'code-davinci-edit-001', 'curie-similarity', 'babbage-search-document', 'curie-instruct-beta', 'text-search-ada-doc-001', 'davinci-instruct-beta', 'text-similarity-babbage-001', 'text-search-davinci-doc-001', 'gpt-4-0314', 'babbage-similarity', 'davinci-search-query', 'text-similarity-curie-001', 'text-davinci-001', 'text-search-davinci-query-001', 'ada-search-document', 'ada-code-search-code', 'babbage-002', 'gpt-4-0613', 'davinci-002', 'davinci-search-document', 'curie-search-document', 'babbage-code-search-code', 'text-search-ada-query-001', 'code-search-ada-text-001', 'babbage-code-search-text', 'code-search-babbage-code-001', 'ada-search-query', 'ada-code-search-text', 'text-search-curie-query-001', 'text-davinci-002', 'text-embedding-ada-002', 'text-davinci-edit-001', 'code-search-babbage-text-001', 'gpt-3.5-turbo-instruct-0914', 'ada', 'text-ada-001', 'ada-similarity', 'code-search-ada-code-001', 'text-similarity-ada-001', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo-instruct', 'text-search-curie-doc-001', 'text-davinci-003', 'text-curie-001', 'curie']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A valid message for OpenAI has two features\n",
    "1. It is valid JSON.\n",
    "1. It has a \"role\".\n",
    "1. It has a \"message\".\n",
    "\n",
    "We can check this with `is_valid_message`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_message(message: Dict[str, Any]) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a single message dictionary has the correct format to be sent to OpenAI.\n",
    "\n",
    "    Args:\n",
    "        message (Dict[str, Any]): A message dictionary with 'role' (str) and 'content' (str) keys.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the message is in the correct format, False otherwise.\n",
    "    \"\"\"\n",
    "    # Check if the message dictionary has 'role' and 'content' keys of the correct types.\n",
    "    if isinstance(message, dict) and 'role' in message and 'content' in message:\n",
    "        if isinstance(message['role'], str) and isinstance(message['content'], str):\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very common to have to check multiple messages, not just one.\n",
    "\n",
    "We can check them all with `are_valid_messages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_valid_messages(messages: List[Dict[str, Any]]) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a list of messages is in the correct format to be sent to OpenAI.\n",
    "\n",
    "    Args:\n",
    "        messages (List[Dict[str, Any]]): A list of message dictionaries.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if all messages are in the correct format, False otherwise.\n",
    "    \"\"\"\n",
    "    return all(is_valid_message(message) for message in messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most chat interactions have a few parts:\n",
    "1.  A list of messages to be processed together.\n",
    "1. An ID for the openAI model to be used.\n",
    "1. How creattive you want the generation to be.\n",
    "1. The maximum number of tokens you want to use.\n",
    "\n",
    "We encapsulate this in `simple_chat`, and get back a JSON object with a number of different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_chat(messages: List[Dict[str, Any]], model: str = 'gpt-3.5-turbo', temperature: float = 0.9, max_tokens: int = 1024) -> str:\n",
    "    \"\"\"\n",
    "    Conduct a simple chat conversation using OpenAI's GPT-3 model.\n",
    "\n",
    "    Args:\n",
    "        messages (List[Dict[str, Any]]): A list of message dictionaries, where each dictionary contains a 'role' (str)\n",
    "            and 'content' (str) key-value pair representing the role of the message sender (e.g., 'system', 'user', 'assistant')\n",
    "            and the content of the message.\n",
    "        model (str, optional): The OpenAI model to use (default is 'gpt-3.5-turbo').\n",
    "        temperature (float, optional): Controls the randomness of the response. Higher values (e.g., 0.9) make the output more random,\n",
    "            while lower values (e.g., 0.2) make it more deterministic. Default is 0.9.\n",
    "        max_tokens (int, optional): The maximum length of the response, measured in tokens. Default is 1024 tokens.\n",
    "\n",
    "    Returns:\n",
    "        str: The response generated by the GPT-3 model.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input messages are not in the correct format.\n",
    "\n",
    "    Example:\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "            {'role': 'user', 'content': 'What's the weather like today?'},\n",
    "        ]\n",
    "        response = simple_chat(messages)\n",
    "        print(response)  # Print the generated response.\n",
    "    \"\"\"\n",
    "\n",
    "    if not messages:\n",
    "        raise ValueError(\"Input messages list cannot be empty.\")\n",
    "\n",
    "    # Check if all messages are in the correct format.\n",
    "    if not are_valid_messages(messages):\n",
    "        raise ValueError(\"Input messages must be in the format [{'role': str, 'content': str}, ...]\")\n",
    "\n",
    "    if model not in open_ai_models:\n",
    "        raise ValueError(f\"{model} is not a valid model name.\")\n",
    "\n",
    "    # Send the messages to OpenAI and get the response\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be very useful to look at the actual details of the response.\n",
    "\n",
    "`show_response_detail` is a convienience function that outputs the details to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_response_detail(response: openai.openai_object.OpenAIObject):\n",
    "    \"\"\"\n",
    "    Extracts and displays details of the first message choice from an OpenAI response object.\n",
    "\n",
    "    This function is designed to work with response objects returned by OpenAI's language models,\n",
    "    specifically with choices that contain messages with 'role' and 'content' attributes.\n",
    "\n",
    "    Args:\n",
    "        response (openai.openai_object.OpenAIObject): The OpenAI response object containing message choices.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Example:\n",
    "        response = openai.Completion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            prompt=\"Translate the following English text to French: 'Hello, world.'\"\n",
    "        )\n",
    "        response_detail(response)\n",
    "    \"\"\"\n",
    "    \n",
    "    ic({response.choices[0].message.role})\n",
    "    ic({response.choices[0].message.content})\n",
    "    ic({response.usage.prompt_tokens})\n",
    "    ic({response.usage.completion_tokens})\n",
    "    ic({response.usage.total_tokens})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 0 - Example 1\n",
    "\n",
    "## Summarizing a Message\n",
    "\n",
    "A good job for Generative AI is generating text.  Here is a code example of exactly how you can do this with OpenAI.\n",
    "\n",
    "** Notes **\n",
    "\n",
    "1. We are using two different roles to definte the messages.\n",
    "1. The response object contains a lot of interesting information in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are arguments that can be pre-defined and passed to the simple_chat function.\n",
    "# they can be changed as needed.\n",
    "simple_chat_args = {\n",
    "    'temperature': 0,\n",
    "    'model': 'gpt-3.5-turbo',\n",
    "    'max_tokens': 2000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a really long message to deal with\n",
    "long_message = \"\"\"Jupiter is the fifth planet from the Sun and the largest in the Solar System. \n",
    "It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System\n",
    " combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations \n",
    " since before recorded history. It is named after the Roman god Jupiter. When viewed from Earth, Jupiter can be bright enough for its \n",
    " reflected light to cast visible shadows, and is on average the third-brightest natural object in the night sky after the Moon and Venus.\"\"\"\n",
    "\n",
    "# build our messages to send to openAI.  These should be well formed JSON with a ROLE and CONTENT\n",
    "system_message = {\"role\":\"system\", \"content\":\"Summarize content you are provided.\"}\n",
    "user_message = {\"role\":\"user\", \"content\":long_message}\n",
    "# send the information to OpenAI and get back a response\n",
    "summary_response = simple_chat(messages=[system_message, user_message], **simple_chat_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-8DOb0D3z1PwiMeSTQQ4im0IreJOaH at 0x114642f90> JSON: {\n",
       "  \"id\": \"chatcmpl-8DOb0D3z1PwiMeSTQQ4im0IreJOaH\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1698203502,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"Jupiter is the largest planet in the Solar System and is located fifth from the Sun. It is a gas giant with a mass that is one-thousandth that of the Sun, but is two-and-a-half times more massive than all the other planets combined. Jupiter has been observed by ancient civilizations and is one of the brightest objects visible to the naked eye in the night sky. It is named after the Roman god Jupiter and is known for its brightness, sometimes even casting visible shadows on Earth. On average, it is the third-brightest natural object in the night sky, after the Moon and Venus.\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 153,\n",
       "    \"completion_tokens\": 123,\n",
       "    \"total_tokens\": 276\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| {response.choices[0].message.role}: {'assistant'}\n",
      "ic| {response.choices[0].message.content}: {'Jupiter is the largest planet in the Solar System and is located fifth from '\n",
      "                                            'the Sun. It is a gas giant with a mass that is one-thousandth that of the '\n",
      "                                            'Sun, but is two-and-a-half times more massive than all the other planets '\n",
      "                                            'combined. Jupiter has been observed by ancient civilizations and is one of '\n",
      "                                            'the brightest objects visible to the naked eye in the night sky. It is named '\n",
      "                                            'after the Roman god Jupiter and is known for its brightness, sometimes even '\n",
      "                                            'casting visible shadows on Earth. On average, it is the third-brightest '\n",
      "                                            'natural object in the night sky, after the Moon and Venus.'}\n",
      "ic| {response.usage.prompt_tokens}: {153}\n",
      "ic| {response.usage.completion_tokens}: {123}\n",
      "ic| {response.usage.total_tokens}: {276}\n"
     ]
    }
   ],
   "source": [
    "show_response_detail(summary_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 0 - Example 2\n",
    "\n",
    "## Classification\n",
    "\n",
    "Another good job for Generative AI is generating classification, or putting things into buckets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| item: 'Sunflower', classification: 'flower'\n",
      "ic| item: 'Carnation', classification: 'flower'\n",
      "ic| item: 'Bluebonnet', classification: 'flower'\n",
      "ic| item: 'Alice', classification: 'person'\n",
      "ic| item: 'Bob', classification: 'person'\n",
      "ic| item: 'Carla', classification: 'person'\n",
      "ic| item: 'Television', classification: 'other'\n",
      "ic| item: 'Barbell', classification: 'other'\n",
      "ic| item: 'Doll', classification: 'Other'\n"
     ]
    }
   ],
   "source": [
    "simple_chat_args = {\n",
    "    'temperature': 0,\n",
    "    'model': 'gpt-3.5-turbo',\n",
    "    'max_tokens': 2000,\n",
    "}\n",
    "\n",
    "# Define a list containing names of flowers\n",
    "flowers = [\"Sunflower\", \"Carnation\", \"Bluebonnet\"]\n",
    "\n",
    "# Define a list containing names of people\n",
    "people = [\"Alice\", \"Bob\", \"Carla\"]\n",
    "\n",
    "# Define a list of random items that aren't categorized as flowers or people\n",
    "random = [\"Television\", \"Barbell\", \"Doll\"]\n",
    "\n",
    "# Combine all the individual lists (flowers, people, random) into one comprehensive list\n",
    "everything = flowers + people + random\n",
    "\n",
    "# Set up an instruction for the system to classify the items in the 'everything' list\n",
    "instructions = \"Classify as one or more types: flower, person, or other.\"\n",
    "system_message = {\"role\": \"system\", \"content\": instructions}\n",
    "\n",
    "# Iterate over each item in the 'everything' list\n",
    "for item in everything:\n",
    "    \n",
    "    # Construct a user message for each item, prompting its classification\n",
    "    user_message = {\"role\": \"user\", \"content\": f\"Classify this: {item}\"}\n",
    "    \n",
    "    # Send the system and user messages to and get back a classification response\n",
    "    classification_response = simple_chat(messages=[system_message, user_message], temperature=0)\n",
    "    \n",
    "    # Extract the content of the response which contains the classification\n",
    "    classification = classification_response.choices[0].message.content\n",
    "    \n",
    "    # Print (or log) the item and its classification\n",
    "    ic(item, classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 0 - Example 3\n",
    "\n",
    "## Generating Data\n",
    "\n",
    "Another good job for Generative AI is generating realistic looking data, either structured or unstructured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| story: ('Once upon a time, in The Oonmay, there were a group of earsbay who lived '\n",
      "            'peacefully among the stars. They were known as the Earbay Clan, and they '\n",
      "            'were led by their wise and gentle leader, Owdgray Earbay. However, one '\n",
      "            'fateful day, a wicked witch named Ellaay cast a spell on the Earbay Clan, '\n",
      "            'turning them into ferocious and wild beasts.\n",
      "           '\n",
      "            '\n",
      "           '\n",
      "            'In the midst of the chaos, a brave and clever erohay named Illylay the '\n",
      "            'ogfray emerged. Illylay was known for her quick thinking and her love for '\n",
      "            'her fellow creatures. Determined to save the Earbay Clan, Illylay set out on '\n",
      "            'a journey to find the ancient and powerful amulet that could break the '\n",
      "            \"witch's spell.\n",
      "           \"\n",
      "            '\n",
      "           '\n",
      "            'With her trusty map and her boundless courage, Illylay hopped from one moon '\n",
      "            'crater to another, facing many challenges along the way. She encountered '\n",
      "            'giant space rocks, treacherous lunar storms, and even a mischievous group of '\n",
      "            'moon rabbits. But Illylay never lost hope, for she knew that the fate of the '\n",
      "            'Earbay Clan depended on her bravery.\n",
      "           '\n",
      "            '\n",
      "           '\n",
      "            'Finally, after a long and perilous journey, Illylay reached the heart of The '\n",
      "            'Oonmay and found the ancient amulet. With a wave of her tiny froggy hand, '\n",
      "            'she broke the spell that had transformed the Earbay Clan. The earsbay '\n",
      "            'returned to their gentle and peaceful nature, forever grateful to their '\n",
      "            'hero, Illylay the ogfray. From that day forward, they lived harmoniously in '\n",
      "            'The Oonmay, under the watchful eyes of their wise leader, Owdgray Earbay, '\n",
      "            'and the brave heart of Illylay.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Once upon a time, in The Oonmay, there were a group of earsbay who lived peacefully among the stars. They were known as the Earbay Clan, and they were led by their wise and gentle leader, Owdgray Earbay. However, one fateful day, a wicked witch named Ellaay cast a spell on the Earbay Clan, turning them into ferocious and wild beasts.\\n\\nIn the midst of the chaos, a brave and clever erohay named Illylay the ogfray emerged. Illylay was known for her quick thinking and her love for her fellow creatures. Determined to save the Earbay Clan, Illylay set out on a journey to find the ancient and powerful amulet that could break the witch's spell.\\n\\nWith her trusty map and her boundless courage, Illylay hopped from one moon crater to another, facing many challenges along the way. She encountered giant space rocks, treacherous lunar storms, and even a mischievous group of moon rabbits. But Illylay never lost hope, for she knew that the fate of the Earbay Clan depended on her bravery.\\n\\nFinally, after a long and perilous journey, Illylay reached the heart of The Oonmay and found the ancient amulet. With a wave of her tiny froggy hand, she broke the spell that had transformed the Earbay Clan. The earsbay returned to their gentle and peaceful nature, forever grateful to their hero, Illylay the ogfray. From that day forward, they lived harmoniously in The Oonmay, under the watchful eyes of their wise leader, Owdgray Earbay, and the brave heart of Illylay.\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the arguments we are going to use\n",
    "simple_chat_args = {\n",
    "    'temperature': 0.5,\n",
    "    'model': 'gpt-3.5-turbo',\n",
    "    'max_tokens': 2000,\n",
    "}\n",
    "\n",
    "# define some parameters for this story\n",
    "subject = \"bears\"\n",
    "hero = \"Lilly the frog\"\n",
    "location = \"The Moon\"\n",
    "\n",
    "# build the story description from the parameters\n",
    "description = f\"Generate a three paragraph story about {subject} that takes place in {location} with a hero named {hero}.\"\n",
    "\n",
    "# create the messages we are going to use to create the story.\n",
    "system_message = {\"role\":\"system\", \"content\":\"You are a helpful assistant who tells creative stories for children.\"}\n",
    "user_message = {\"role\":\"user\", \"content\": description}\n",
    "\n",
    "# send the information to OpenAI and get back a response\n",
    "story_response = simple_chat(messages=[system_message, user_message], **simple_chat_args)\n",
    "# extract the response from the larger JSON object that was returned\n",
    "story = story_response.choices[0].message.content\n",
    "\n",
    "ic(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same thing with more structured data, like a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| csv: ('id,name,email address,salary,hire date\n",
      "         '\n",
      "          '1,John Doe,johndoe@example.com,50000,2021-01-01\n",
      "         '\n",
      "          '2,Jane Smith,janesmith@example.com,55000,2020-03-15\n",
      "         '\n",
      "          '3,Michael Johnson,michaeljohnson@example.com,60000,2020-07-10\n",
      "         '\n",
      "          '4,Sarah Brown,sarahbrown@example.com,55000,2019-11-20\n",
      "         '\n",
      "          '5,Robert Davis,robertdavis@example.com,65000,2020-06-01\n",
      "         '\n",
      "          '6,Emily Wilson,emilywilson@example.com,50000,2021-02-28\n",
      "         '\n",
      "          '7,David Thompson,davidthompson@example.com,55000,2020-01-15\n",
      "         '\n",
      "          '8,Amanda Anderson,amandaanderson@example.com,60000,2019-05-10\n",
      "         '\n",
      "          '9,James Taylor,jamestaylor@example.com,50000,2021-03-20\n",
      "         '\n",
      "          '10,Laura Martinez,lauramartinez@example.com,55000,2020-09-05')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'id,name,email address,salary,hire date\\n1,John Doe,johndoe@example.com,50000,2021-01-01\\n2,Jane Smith,janesmith@example.com,55000,2020-03-15\\n3,Michael Johnson,michaeljohnson@example.com,60000,2020-07-10\\n4,Sarah Brown,sarahbrown@example.com,55000,2019-11-20\\n5,Robert Davis,robertdavis@example.com,65000,2020-06-01\\n6,Emily Wilson,emilywilson@example.com,50000,2021-02-28\\n7,David Thompson,davidthompson@example.com,55000,2020-01-15\\n8,Amanda Anderson,amandaanderson@example.com,60000,2019-05-10\\n9,James Taylor,jamestaylor@example.com,50000,2021-03-20\\n10,Laura Martinez,lauramartinez@example.com,55000,2020-09-05'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the arguments we are goint to use\n",
    "simple_chat_args = {\n",
    "    'temperature': 0.8,\n",
    "    'model': 'gpt-3.5-turbo',\n",
    "    'max_tokens': 2000,\n",
    "}\n",
    "\n",
    "description = \"Generate a list of 10 people who work in an office. Include id, name, email address, salary, and hire date.\"\n",
    "\n",
    "# create the messages we are going to use to create the story.\n",
    "system_message = {\"role\":\"system\", \"content\":\"You are a helpful assistant who generates CSV data for spreadsheets\"}\n",
    "user_message = {\"role\":\"user\", \"content\": description}\n",
    "\n",
    "# send the information to OpenAI and get back a response\n",
    "csv_response = simple_chat(messages=[system_message, user_message])\n",
    "# extract the response from the larger JSON object that was returned\n",
    "csv = csv_response.choices[0].message.content\n",
    "\n",
    "ic(csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
